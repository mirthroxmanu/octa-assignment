# OctaByte Infrastructure - DevOps Assignment

Complete Infrastructure as Code (IaC) solution for deploying a application on AWS with EKS, RDS PostgreSQL, comprehensive monitoring, and CI/CD pipelines.

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
- [Prerequisites](#prerequisites)
- [Quick Start Guide](#quick-start-guide)
- [Infrastructure Components](#infrastructure-components)
- [CI/CD Pipeline](#cicd-pipeline)
- [Monitoring Stack](#monitoring-stack)
- [Security Considerations](#security-considerations)
- [Cost Optimization](#cost-optimization)
- [Troubleshooting](#troubleshooting)

## ğŸ¯ Overview

This project implements a complete DevOps infrastructure solution featuring:

- **Infrastructure as Code**: Terraform modules for AWS resources
- **Container Orchestration**: Amazon EKS (Elastic Kubernetes Service)
- **Database**: RDS PostgreSQL with automated backups
- **Monitoring**: Grafana + Loki (Logs) + Mimir (Metrics)
- **CI/CD**: GitHub Actions with multi-environment deployment
- **Security**: VPN server, security groups, IAM roles, encrypted secrets
- **High Availability**: Multi-AZ deployment across ap-south-1a and ap-south-1b

### Technology Stack

| Component | Technology | Version |
|-----------|-----------|---------|
| **Cloud Provider** | AWS | - |
| **Region** | ap-south-1 (Mumbai) | - |
| **IaC** | Terraform | >= 1.0 |
| **Container Orchestration** | Amazon EKS | 1.33 |
| **Database** | RDS PostgreSQL | 17.2 |
| **CI/CD** | GitHub Actions | - |
| **Monitoring** | Grafana + Loki + Mimir | Latest |
| **Load Balancer** | AWS ALB Controller | Latest |
| **Storage** | Amazon S3 + EBS CSI | - |

## ğŸ—ï¸ Architecture

### High-Level Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Internet / Users                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Application Load    â”‚
              â”‚     Balancer (ALB)   â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚               â”‚               â”‚
         â–¼               â–¼               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VPC (10.10.0.0/16)                           â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  Public Subnet-1      â”‚      â”‚  Public Subnet-2      â”‚      â”‚
â”‚  â”‚  10.10.0.0/24         â”‚      â”‚  10.10.16.0/24        â”‚      â”‚
â”‚  â”‚  (ap-south-1a)        â”‚      â”‚  (ap-south-1b)        â”‚      â”‚
â”‚  â”‚                       â”‚      â”‚                       â”‚      â”‚
â”‚  â”‚  â€¢ NAT Gateway        â”‚      â”‚  â€¢ NAT Gateway        â”‚      â”‚
â”‚  â”‚  â€¢ VPN Server         â”‚      â”‚  â€¢ Bastion (optional) â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚              â”‚                               â”‚                  â”‚
â”‚              â–¼                               â–¼                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  Private Subnet-1     â”‚      â”‚  Private Subnet-2     â”‚      â”‚
â”‚  â”‚  10.10.32.0/24        â”‚      â”‚  10.10.48.0/24        â”‚      â”‚
â”‚  â”‚  (ap-south-1a)        â”‚      â”‚  (ap-south-1b)        â”‚      â”‚
â”‚  â”‚                       â”‚      â”‚                       â”‚      â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚      â”‚
â”‚  â”‚  â”‚  EKS Node Pool  â”‚  â”‚      â”‚  â”‚  EKS Node Pool  â”‚  â”‚      â”‚
â”‚  â”‚  â”‚  m6g.xlarge     â”‚  â”‚      â”‚  â”‚  m6g.xlarge     â”‚  â”‚      â”‚
â”‚  â”‚  â”‚  (ARM64)        â”‚  â”‚      â”‚  â”‚  (ARM64)        â”‚  â”‚      â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚      â”‚
â”‚  â”‚           â”‚            â”‚      â”‚           â”‚            â”‚      â”‚
â”‚  â”‚           â–¼            â”‚      â”‚           â”‚            â”‚      â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚      â”‚
â”‚  â”‚  â”‚  RDS PostgreSQL â”‚  â”‚      â”‚  â”‚   Monitoring    â”‚  â”‚      â”‚
â”‚  â”‚  â”‚    (Primary)    â”‚  â”‚      â”‚  â”‚   Workloads     â”‚  â”‚      â”‚
â”‚  â”‚  â”‚   db.t4g.micro  â”‚  â”‚      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚      â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚      â”‚                       â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚    Amazon S3         â”‚
              â”‚  â€¢ Loki Logs         â”‚
              â”‚  â€¢ Mimir Metrics     â”‚
              â”‚  â€¢ Terraform State   â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Network Design

- **VPC CIDR**: 10.10.0.0/16
- **Public Subnets**: 
  - 10.10.0.0/24 (ap-south-1a)
  - 10.10.16.0/24 (ap-south-1b)
- **Private Subnets**: 
  - 10.10.32.0/24 (ap-south-1a)
  - 10.10.48.0/24 (ap-south-1b)
- **High Availability**: Resources distributed across 2 AZs
- **NAT Gateways**: One per AZ for private subnet internet access

### EKS Cluster Configuration

- **Cluster Version**: 1.33
- **Node Group**: `common-ng`
  - **Instance Type**: m6g.xlarge (ARM64 - Graviton2)
  - **AMI**: AL2023_ARM_64_STANDARD
  - **Capacity**: Min: 2, Desired: 1, Max: 6
  - **Disk**: 60GB EBS
  - **Capacity Type**: ON_DEMAND
- **Addons**:
  - VPC CNI: v1.20.2
  - EBS CSI Driver: v1.48.0
  - CoreDNS: v1.12.4
  - Kube-proxy: v1.33.3

## ğŸ“‹ Prerequisites

### Required Tools

Install the following tools before starting:
```bash
# Terraform
brew install terraform  # macOS
# or
wget https://releases.hashicorp.com/terraform/1.6.0/terraform_1.6.0_linux_amd64.zip

# AWS CLI
brew install awscli  # macOS
# or
pip install awscli

# kubectl
brew install kubectl  # macOS
# or
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"

# Helm
brew install helm  # macOS
# or
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
```

### AWS Account Setup

1. **Create AWS Account**: Sign up at [aws.amazon.com](https://aws.amazon.com)

2. **Configure AWS CLI**:
```bash
aws configure
# Enter your:
# - AWS Access Key ID
# - AWS Secret Access Key
# - Default region: ap-south-1
# - Default output format: json
```

3. **Verify Access**:
```bash
aws sts get-caller-identity
```

### Required AWS Permissions

Your IAM user/role needs the following permissions:
- EC2 (VPC, Security Groups, Instances)
- EKS (Cluster management)
- RDS (Database management)
- S3 (Bucket management)
- IAM (Role and Policy management)
- KMS (Key management for secrets)
- CloudWatch (Logging and monitoring)

## ğŸš€ Quick Start Guide

### Step 1: Clone the Repository
```bash
git clone <your-repository-url>
cd octa-byte-assignment/accounts/octa-demo
```

### Step 2: Set Up Encrypted Credentials

The project uses AWS KMS for secret encryption. Create your encrypted credentials:
```bash
# Create the credentials directory
mkdir -p ../../creds-encrypted

# Create RDS credentials file
cat > rds_db_creds.yml <<EOF
rds_master_user: admin
rds_master_password: YourSecurePassword123!
EOF

# Encrypt the credentials using AWS KMS
aws kms encrypt \
  --key-id alias/your-kms-key \
  --plaintext fileb://rds_db_creds.yml \
  --output text \
  --query CiphertextBlob | base64 --decode > ../../creds-encrypted/rds_db_creds.yml.encrypted

# Remove plaintext file
rm rds_db_creds.yml
```

### Step 3: Configure Terraform Variables

Create a `terraform.tfvars` file in the `core` directory:
```bash
cd core

cat > terraform.tfvars <<EOF
# Common Configuration
account = "octabyte"
env     = "production"

# Network Configuration
vpc_cidr_block     = "10.10.0.0/16"
public_subnets     = ["10.10.0.0/24", "10.10.16.0/24"]
private_subnets    = ["10.10.32.0/24", "10.10.48.0/24"]
availability_zones = ["ap-south-1a", "ap-south-1b"]
aws_region         = "ap-south-1"

# SSH Key (generate if you don't have one)
sshkey_master_pub = "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDxxxxx..."
EOF
```

### Step 4: Initialize Terraform
```bash
# Initialize Terraform and download providers
terraform init

# Validate configuration
terraform validate

# Review execution plan
terraform plan
```

### Step 5: Deploy Core Infrastructure
```bash
# Apply the infrastructure
terraform apply

# Type 'yes' when prompted
```

This will create:
- VPC with public and private subnets
- NAT Gateways
- Security Groups
- EKS Cluster with node group
- RDS PostgreSQL instance
- S3 buckets for logs and metrics
- IAM roles and policies
- VPN server

**Deployment time**: ~15-20 minutes

### Step 6: Configure kubectl
```bash
# Update kubeconfig to access the EKS cluster
aws eks update-kubeconfig \
  --name octabyte-octabyte-eks-cluster \
  --region ap-south-1

# Verify cluster access
kubectl get nodes

# Should show 2 nodes (minimum)
```

### Step 7: Deploy Kubernetes Resources
```bash
cd ../k8s

# Initialize Terraform for k8s resources
terraform init

# Deploy monitoring stack and controllers
terraform apply
```

This will deploy:
- AWS Load Balancer Controller
- Grafana (monitoring UI)
- Loki (log aggregation)
- Mimir (metrics storage)
- Kubernetes Dashboard (optional)

### Step 8: Access Monitoring Dashboards
```bash
# Get Grafana admin password
kubectl get secret --namespace monitoring grafana -o jsonpath="{.data.admin-password}" | base64 --decode

# Port-forward Grafana
kubectl port-forward --namespace monitoring svc/grafana 3000:80

# Access Grafana at: http://localhost:3000
# Username: admin
# Password: <from above command>
```

## ğŸ”§ Infrastructure Components

### 1. Networking (VPC)

**File**: `core/vpc.tf`

- Custom VPC with dual NAT gateways for high availability
- Public subnets for load balancers and VPN
- Private subnets for application workloads and databases
- Internet Gateway for public subnet internet access
- Route tables configured for proper traffic flow

### 2. EKS Cluster

**File**: `core/eks.tf`

**Key Features**:
- Managed Kubernetes control plane (v1.33)
- ARM64-based nodes for cost optimization (Graviton2)
- Auto-scaling enabled (2-6 nodes)
- Multiple addons pre-configured
- IRSA (IAM Roles for Service Accounts) enabled

**Node Pool Configuration**:
```hcl
Instance Type: m6g.xlarge
vCPU: 4
Memory: 16 GB
Architecture: ARM64
Disk: 60 GB gp3
```

### 3. RDS PostgreSQL

**File**: `core/psql-rds.tf`

**Configuration**:
- Engine: PostgreSQL 17.2
- Instance Class: db.t4g.micro (Graviton2)
- Storage: 50GB (auto-scaling to 100GB)
- Backup Retention: 7 days
- Multi-AZ: Disabled (single AZ for cost optimization in demo)
- Encryption: At-rest encryption enabled
- Deletion Protection: Enabled
- Performance Insights: Enabled

**Security**:
- Deployed in private subnet
- Not publicly accessible
- Custom security group with VPC-only access
- Automated backups with point-in-time recovery

### 4. Security Groups

**File**: `core/security_groups.tf`

**Security Groups Created**:

1. **EKS Cluster Security Group**:
   - HTTPS (443) from VPC CIDR
   - All traffic from load balancer SGs

2. **Public Network SG** (VPN/Bastion):
   - HTTP (80) from anywhere
   - HTTPS (443) from anywhere
   - UDP 11024 for VPN
   - SSH (22) from specific IP

3. **Private Network SG** (Applications):
   - SSH (22) from VPN server only

4. **Load Balancer SGs**:
   - HTTP (80) and HTTPS (443) from anywhere
   - Separate SGs for public and internal LBs

5. **RDS Security Group**:
   - PostgreSQL (5432) from VPC CIDR only

### 5. S3 Buckets

**File**: `core/s3-buckets.tf`

- **loki-logs-bucket**: Stores application and system logs
- **mimir-backend-bucket**: Stores Prometheus metrics
- Both configured with:
  - Block public access
  - Versioning enabled
  - Server-side encryption

### 6. IAM Roles

**Files**: 
- `core/loadbalancercontroller-iam-role.tf`
- `core/loki-iam-role.tf`
- `core/mimir-iam-role.tf`

**IRSA Roles Created**:
1. **AWS Load Balancer Controller**: Manages ALB/NLB creation
2. **Loki**: Write access to S3 logs bucket
3. **Mimir**: Write access to S3 metrics bucket

All use OIDC provider for secure service account authentication.

### 7. State Management

**File**: `core/backend.tf`
```hcl
Backend: S3
Bucket: octabyte-terraform-state-backend
Key: octabyte/terraform.tfstate
Region: ap-south-1
```

**Benefits**:
- Remote state storage
- State locking (with DynamoDB)
- Team collaboration
- State versioning

## ğŸ”„ CI/CD Pipeline

### GitHub Actions Workflow

**File**: `CICD/workflow.yaml`

### Pipeline Stages
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Setup      â”‚  â†’ Determine environment (staging/qa)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Build      â”‚  â†’ Build Docker image
â”‚              â”‚  â†’ Push to ECR
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Deploy     â”‚  â†’ Deploy to EKS via Helm
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Workflow Triggers

- **Staging**: Push to `staging` branch
- **QA**: Push to `qa` branch
- **Production**: Manual approval required (to be implemented)

### Build Process

1. **Checkout Code**: Clone repository
2. **Generate Image Tag**: Format: `DD-MM-YYYY-<run_number>`
3. **Login to ECR**: Authenticate with Amazon ECR
4. **Build Image**: 
```bash
   docker build -t $IMAGE_URI .
```
5. **Push Image**: Push to environment-specific ECR repository
6. **Output**: Image URI and tag for deployment stage

### Deploy Process

1. **Update kubeconfig**: Configure kubectl for target EKS cluster
2. **Checkout Helm Charts**: Clone chart repository
3. **Inject Environment Variables**: Create temporary values file
4. **Helm Upgrade**: 
```bash
   helm upgrade --install $APP_NAME \
     ./helm-charts/app-base-common/source/octa \
     --set image.repository=$IMAGE_URI \
     --set image.tag=$IMAGE_TAG \
     --namespace $ENVIRONMENT \
     --timeout 15m
```

### Environment Configuration

| Environment | Branch | Cluster | Namespace |
|-------------|--------|---------|-----------|
| Staging | `staging` | staging-cluster | staging |
| QA | `qa` | qa-cluster | qa |
| Production | `main` | prod-cluster | production |

### Required Secrets

Configure these in GitHub repository secrets:
```yaml
Secrets Required:
- AWS_ACCESS_KEY_ID
- AWS_SECRET_ACCESS_KEY
- GIT_PAT (for private chart repository access)
- SLACK_WEBHOOK (optional, for notifications)
```

### Best Practices Implemented

1. **Environment Segregation**: Separate clusters per environment
2. **Immutable Tags**: Date + run number prevents overwriting
3. **Namespace Isolation**: Each environment has its own namespace
4. **Helm Values**: Environment-specific configurations
5. **Timeout Protection**: 15-minute deployment timeout
6. **Automatic Rollback**: Helm rollback on failure

## ğŸ“Š Monitoring Stack

### Architecture
```
Application â†’ Logs â†’ Loki â†’ S3
             â†“
          Metrics â†’ Mimir â†’ S3
             â†“
        Grafana Dashboard
```

### Components

#### 1. Grafana (Visualization)

**Deployment**: `k8s/helm-grafana.tf`

- **Version**: 10.1.2
- **Namespace**: monitoring
- **Purpose**: Unified dashboard for metrics and logs
- **Access**: Port-forward or Ingress

**Features**:
- Pre-configured data sources (Loki, Mimir)
- Custom dashboards for infrastructure metrics
- Alerting capabilities
- User management

#### 2. Loki (Log Aggregation)

**Deployment**: `k8s/helm-loki.tf`

- **Version**: 6.44.0
- **Namespace**: loki
- **Backend**: S3 bucket (`octabyte-octabyte-loki-logs-bucket`)
- **Purpose**: Centralized log storage and querying

**Configuration**:
- Write logs to S3 for long-term retention
- IRSA for S3 access (no static credentials)
- LogQL query language support
- Label-based log filtering

#### 3. Mimir (Metrics Storage)

**Deployment**: `k8s/helm-mimir.tf`

- **Version**: 5.6.1
- **Namespace**: mimir
- **Backend**: S3 bucket (`octabyte-octabyte-mimir-backend-bucket`)
- **Purpose**: Long-term Prometheus metrics storage

**Configuration**:
- Horizontally scalable metrics storage
- PromQL compatible
- S3 backend for cost-effective storage
- High availability setup

### Monitoring Capabilities

1. **Infrastructure Metrics**:
   - CPU, Memory, Disk usage per node
   - Network I/O
   - Pod resource consumption
   - Container metrics

2. **Application Metrics**:
   - Request rate
   - Error rate
   - Response time/latency
   - Custom application metrics

3. **Database Metrics**:
   - RDS Performance Insights
   - Connection count
   - Query performance
   - Disk I/O

4. **Kubernetes Metrics**:
   - Pod status and restarts
   - Deployment rollout status
   - Resource quotas
   - PV/PVC usage

### Accessing Monitoring
```bash
# Port-forward Grafana
kubectl port-forward -n monitoring svc/grafana 3000:80

# Access at http://localhost:3000
# Default credentials: admin / <retrieve from secret>
```

## ğŸ”’ Security Considerations

### 1. Network Security

**VPC Isolation**:
- Private subnets for all application workloads
- Public subnets only for load balancers and VPN
- No direct internet access to private resources
- All outbound traffic through NAT Gateways

**Security Groups (Defense in Depth)**:
- Principle of least privilege applied
- Separate SGs for each component
- Source-based rules (no 0.0.0.0/0 for internal services)
- Port-specific rules (no broad port ranges)

**Network Segmentation**:
```
Internet
    â†“
Load Balancer (Public Subnet)
    â†“
EKS Nodes (Private Subnet)
    â†“
RDS Database (Private Subnet - No internet access)
```

### 2. Access Control

**VPN Server**:
- Deployed in public subnet (10.10.0.4)
- All SSH access only from VPN IP
- UDP port 11024 for VPN connections
- Acts as bastion for private resources

**IAM Roles & IRSA**:
- No static AWS credentials in pods
- Service accounts mapped to IAM roles
- Scoped permissions per workload
- Automatic credential rotation

**SSH Access**:
- Public key authentication only
- Password authentication disabled
- SSH access restricted to VPN IP (10.10.0.4)
- Separate SGs for public vs private SSH access

### 3. Data Security

**Encryption at Rest**:
- RDS: Encrypted using AWS KMS
- S3: Server-side encryption (SSE-S3)
- EBS Volumes: Encrypted by default

**Encryption in Transit**:
- TLS for all ALB connections
- Internal cluster communication encrypted
- RDS connections use SSL/TLS

**Secret Management** (See [SECRET_MANAGEMENT.md](./SECRET_MANAGEMENT.md)):
- AWS KMS for secret encryption
- Encrypted credentials never in version control
- Terraform reads encrypted files at apply time
- Database passwords rotated regularly

### 4. Database Security

**RDS PostgreSQL**:
- Not publicly accessible
- Deployed in private subnet
- Security group allows only VPC access (10.10.0.0/16)
- Deletion protection enabled
- Automated backups encrypted
- SSL/TLS required for connections

### 5. Container Security

**EKS Security**:
- Private API endpoint (accessible only from VPC)
- Node groups in private subnets
- Pod Security Standards enforced
- Network policies for pod-to-pod communication
- RBAC configured for fine-grained access

**Image Security**:
- Container images scanned for vulnerabilities
- Private ECR registry
- Image pull secrets managed via Kubernetes
- Only signed and approved images deployed

### 6. Compliance & Auditing

**CloudTrail**:
- All API calls logged
- S3 bucket for log storage
- Log file integrity validation

**VPC Flow Logs**:
- Network traffic logging
- Stored in CloudWatch Logs
- Helps with security investigations

**RDS Audit Logging**:
- Database activity logging enabled
- Connection attempts logged
- Query logging for sensitive operations

### Security Best Practices Implemented

âœ… **Principle of Least Privilege**: Minimal required permissions
âœ… **Defense in Depth**: Multiple security layers
âœ… **Encryption**: At rest and in transit
âœ… **Network Segmentation**: Public/private subnet separation
âœ… **Secure Access**: VPN-only access to private resources
âœ… **Audit Logging**: Comprehensive activity logging
âœ… **Automated Patching**: Enabled for RDS and EKS
âœ… **Secret Rotation**: Regular credential updates
âœ… **No Hardcoded Secrets**: All secrets encrypted or in secrets manager

## ğŸ’° Cost Optimization

### Current Infrastructure Costs (Monthly Estimate)

| Resource | Type | Count | Unit Cost | Total |
|----------|------|-------|-----------|-------|
| **EKS Cluster** | Control Plane | 1 | $73 | $73 |
| **EC2 Instances** | m6g.xlarge (ARM) | 2-6 | $0.154/hr | $222-667 |
| **RDS PostgreSQL** | db.t4g.micro (ARM) | 1 | $0.016/hr | $12 |
| **NAT Gateway** | Multi-AZ | 2 | $0.045/hr + data | $65 + data |
| **Load Balancers** | Application LB | 2-3 | $0.0225/hr | $33-50 |
| **EBS Volumes** | gp3 (60GB/node) | 2-6 | $0.08/GB | $10-29 |
| **S3 Storage** | Standard | ~100GB | $0.023/GB | $2-5 |
| **Data Transfer** | Out to internet | Variable | $0.09/GB | $10-50 |
| **VPN Server** | t3.micro | 1 | $0.0104/hr | $8 |
| | | | **Total** | **$435-$939/month** |

*Estimated range based on 2-6 node scaling*

### Cost Optimization Strategies Implemented

#### 1. ARM-based Instances (Graviton2)

**Savings**: ~20% compared to x86 instances
```hcl
# EKS Nodes
instance_types = ["m6g.xlarge"]  # ARM64
ami_type = "AL2023_ARM_64_STANDARD"

# RDS
instance_class = "db.t4g.micro"  # ARM64
```

**Benefits**:
- Lower hourly rates
- Better price-performance ratio
- Same or better performance for most workloads

#### 2. Auto-Scaling
```hcl
desired_size = 1
min_size     = 2
max_size     = 6
```

**Savings**: ~40-60% during low-traffic periods

- Automatically scales down during off-peak hours
- Scales up during high demand
- Max unavailable: 25% for zero-downtime updates

#### 3. Spot Instances (Optional Enhancement)
```hcl
# Can be added to node group
capacity_type = "SPOT"  # Instead of ON_DEMAND
```

**Potential Savings**: Up to 90% for fault-tolerant workloads

#### 4. Storage Optimization

**S3 Lifecycle Policies**:
```hcl
# Recommended addition to S3 buckets
lifecycle_rule {
  enabled = true
  transition {
    days          = 30
    storage_class = "STANDARD_IA"  # Cheaper for infrequent access
  }
  transition {
    days          = 90
    storage_class = "GLACIER"  # Much cheaper for archives
  }
}
```

**EBS Volume Type**:
- Using gp3 instead of gp2 (20% cheaper)
- Right-sized at 60GB per node

#### 5. RDS Optimization

**Current**:
- db.t4g.micro (smallest production-capable)
- Single-AZ (Multi-AZ doubles cost)
- Auto-scaling storage (pay for what you use)

**Storage Auto-Scaling**:
```hcl
allocated_storage     = 50   # Start small
max_allocated_storage = 100  # Grow as needed
```

#### 6. Network Optimization

**Data Transfer**:
- Use CloudFront CDN for static assets (cheaper than ALB data transfer)
- Keep traffic within same AZ when possible
- Use VPC endpoints for AWS services (avoid NAT Gateway charges)

**NAT Gateway Alternatives**:
- Current: 2 NAT Gateways ($65/month + data)
- Alternative: NAT Instances (cheaper but less managed)
- Future: Single NAT Gateway (save $32.50/month, lose HA)

#### 7. Reserved Instances & Savings Plans

**Recommendations** (not currently implemented):
```bash
# For predictable workloads, commit to 1-3 years:

# EC2 Savings Plan (Compute Savings Plan)
# Save up to 72% on m6g.xlarge
Current cost: $222/month (2 nodes)
With 1-year commitment: ~$156/month
With 3-year commitment: ~$133/month

# RDS Reserved Instance
Current cost: $12/month
With 1-year commitment: ~$8/month
With 3-year commitment: ~$6/month
```

#### 8. Monitoring Cost Control

**S3 Storage Management**:
```bash
# Loki and Mimir logs can grow quickly
# Implement retention policies:

# Keep detailed logs for 7 days
# Keep aggregated logs for 30 days
# Keep monthly summaries for 12 months
# Delete after 12 months
```

**Estimated S3 Costs**:
- Without lifecycle: $50-100/month
- With lifecycle: $5-15/month
- **Savings**: ~80-90%

#### 9. Development Environment Optimization

**Stop Non-Production Resources**:
```bash
# Script to stop staging/qa during nights and weekends
# Potential savings: 60% of staging costs

# Example: Stop staging cluster on Friday 6 PM
# Restart on Monday 8 AM
# Save: 62 hours/week = ~37% of time
```

#### 10. Load Balancer Consolidation

**Current**: Multiple ALBs (~$33-50/month each)

**Optimization**:
- Use single ALB with multiple target groups
- Use host-based or path-based routing
- **Potential Savings**: $33-100/month

### Cost Optimization Roadmap

**Immediate** (0-30 days):
- âœ… ARM-based instances (implemented)
- âœ… Auto-scaling (implemented)
- âœ… gp3 volumes (implemented)
- â¬œ S3 lifecycle policies
- â¬œ Stop dev/staging on schedule

**Short-term** (1-3 months):
- â¬œ Reserved Instances for stable workloads
- â¬œ Consolidate load balancers
- â¬œ Implement CloudFront CDN
- â¬œ VPC endpoints for AWS services

**Long-term** (3-12 months):
- â¬œ Evaluate Fargate for some workloads
- â¬œ Move to Spot instances where possible
- â¬œ 3-year Savings Plans
- â¬œ Multi-region optimization

### Monitoring Costs

**Enable AWS Cost Explorer**:
```bash
# Tag all resources for cost allocation
tags = {
  Project     = "octabyte"
  Environment = "production"
  ManagedBy   = "terraform"
  CostCenter  = "engineering"
}
```

**Set up Budget Alerts**:
```bash
aws budgets create-budget \
  --account-id 123456789012 \
  --budget file://monthly-budget.json \
  --notifications-with-subscribers file://notifications.json
```

### Cost Optimization Best Practices

1. **Right-size regularly**: Review CloudWatch metrics monthly
2. **Delete unused resources**: Remove old snapshots, unused volumes
3. **Use tagging**: Track costs by team/project/environment
4. **Enable Cost Explorer**: Visualize spending trends
5. **Set budget alerts**: Get notified at 50%, 80%, 100% of budget
6. **Review monthly**: Schedule cost review meetings
7. **Automate cleanup**: Delete old logs, unused images, zombie resources

## ğŸ”§ Troubleshooting

### Common Issues and Solutions

#### Issue 1: Terraform Apply Fails - State Lock

**Error**:
```
Error: Error locking state: Error acquiring the state lock
```

**Solution**:
```bash
# If you're sure no one else is running Terraform:
terraform force-unlock <lock-id>

# Or manually release DynamoDB lock
aws dynamodb delete-item \
  --table-name terraform-state-lock \
  --key '{"LockID": {"S": "octabyte-terraform-state-backend/octabyte/terraform.tfstate"}}'
```

#### Issue 2: EKS Nodes Not Joining Cluster

**Symptoms**:
```bash
kubectl get nodes
# Shows 0 nodes or nodes stuck in NotReady
```

**Solution**:
```bash
# Check node group status
aws eks describe-nodegroup \
  --cluster-name octabyte-octabyte-eks-cluster \
  --nodegroup-name common-ng

# Check security groups allow cluster communication
# Verify IAM role permissions
# Check CloudWatch logs for node group

# Force node group update
aws eks update-nodegroup-config \
  --cluster-name octabyte-octabyte-eks-cluster \
  --nodegroup-name common-ng \
  --scaling-config minSize=2,maxSize=6,desiredSize=3
```

#### Issue 3: Cannot Connect to RDS

**Error**:
```
could not connect to server: Connection timed out
```

**Solution**:
```bash
# Verify security group allows access from source
aws ec2 describe-security-groups \
  --group-ids sg-xxxxx \
  --query 'SecurityGroups[*].IpPermissions'

# Test from EKS node
kubectl run -it --rm debug --image=postgres:17 --restart=Never -- \
  psql -h octabyte-common.xxxxx.ap-south-1.rds.amazonaws.com -U admin -d postgres

# Check RDS is in correct subnet group
# Verify VPC routing tables
```

#### Issue 4: Load Balancer Controller Not Creating ALB

**Symptoms**:
```bash
kubectl get ingress
# Shows ADDRESS column empty
```

**Solution**:
```bash
# Check controller logs
kubectl logs -n kube-system deployment/aws-load-balancer-controller

# Common issues:
# 1. IAM role not attached correctly
kubectl describe sa -n kube-system aws-load-balancer-controller

# 2. Missing subnet tags
aws ec2 describe-subnets --subnet-ids subnet-xxxxx \
  --query 'Subnets[*].Tags'

# Subnets need these tags:
# Public: kubernetes.io/role/elb=1
# Private: kubernetes.io/role/internal-elb=1

# 3. Reinstall controller
helm uninstall aws-load-balancer-controller -n kube-system
terraform apply -target=helm_release.aws_load_balancer_controller
```

#### Issue 5: Grafana Not Showing Data

**Solution**:
```bash
# Check Loki is running
kubectl get pods -n loki
kubectl logs -n loki loki-0

# Check Mimir is running
kubectl get pods -n mimir
kubectl logs -n mimir mimir-0

# Verify S3 bucket permissions
kubectl describe sa -n loki loki
kubectl describe sa -n mimir mimir

# Test data source in Grafana
# Grafana â†’ Configuration â†’ Data Sources â†’ Loki â†’ Test

# Check for IRSA annotation
kubectl get sa loki -n loki -o yaml | grep eks.amazonaws.com/role-arn
```

#### Issue 6: High NAT Gateway Costs

**Solution**:
```bash
# Identify top talkers
aws cloudwatch get-metric-statistics \
  --namespace AWS/NATGateway \
  --metric-name BytesOutToDestination \
  --dimensions Name=NatGatewayId,Value=nat-xxxxx \
  --start-time 2024-01-01T00:00:00Z \
  --end-time 2024-01-07T00:00:00Z \
  --period 3600 \
  --statistics Sum

# Solutions:
# 1. Use VPC endpoints for AWS services
# 2. Use S3 gateway endpoint (free)
# 3. Consolidate to single NAT gateway (lose HA)
# 4. Cache frequently accessed external data
```

#### Issue 7: Pod Cannot Pull Image from ECR

**Error**:
```
Failed to pull image: pull access denied
```

**Solution**:
```bash
# Verify ECR repository exists
aws ecr describe-repositories --repository-names production/test-app

# Check node IAM role has ECR permissions
aws iam get-role-policy \
  --role-name octabyte-octabyte-eks-node-group-role \
  --policy-name ECRReadPolicy

# Manually test image pull from node
kubectl debug node/ip-10-10-32-x -it --image=busybox
# From node:
aws ecr get-login-password --region ap-south-1 | \
  docker login --username AWS --password-stdin \
  123456789012.dkr.ecr.ap-south-1.amazonaws.com
```

#### Issue 8: Terraform Destroys Wrong Resources

**Prevention**:
```bash
# Always use workspaces for environments
terraform workspace list
terraform workspace select production

# Use lifecycle rules to prevent destruction
resource "aws_db_instance" "main" {
  lifecycle {
    prevent_destroy = true
  }
}

# Always review plan before apply
terraform plan -out=tfplan
# Review tfplan
terraform apply tfplan
```

#### Issue 9: Out of IP Addresses in Subnet

**Error**:
```
Error launching instances: Insufficient capacity
```

**Solution**:
```bash
# Check available IPs
aws ec2 describe-subnets --subnet-ids subnet-xxxxx \
  --query 'Subnets[*].AvailableIpAddressCount'

# Solutions:
# 1. Add more subnets in other AZs
# 2. Use larger CIDR blocks
# 3. Clean up unused ENIs
aws ec2 describe-network-interfaces \
  --filters "Name=status,Values=available" \
  --query 'NetworkInterfaces[*].NetworkInterfaceId'

# Delete unused ENIs
aws ec2 delete-network-interface --network-interface-id eni-xxxxx
```

#### Issue 10: GitHub Actions Deploy Fails

**Error**:
```
Error: Kubernetes cluster unreachable
```

**Solution**:
```bash
# Verify GitHub runner has AWS credentials
# Check AWS credentials in repository secrets

# Test kubectl locally with same credentials
export AWS_ACCESS_KEY_ID=xxxxx
export AWS_SECRET_ACCESS_KEY=xxxxx
aws eks update-kubeconfig --name octabyte-octabyte-eks-cluster

# Verify EKS cluster endpoint is accessible
# Check EKS cluster security groups allow GitHub runner IPs

# Update workflow to use correct cluster name
# Ensure kubeconfig is properly created in workflow
```

### Getting Help

1. **Check CloudWatch Logs**:
   - EKS Control Plane logs
   - Application logs via Loki
   - RDS logs

2. **AWS Support**:
   - Support Center: https://console.aws.amazon.com/support/
   - AWS Forums: https://forums.aws.amazon.com/

3. **Terraform Issues**:
   - GitHub Issues: https://github.com/hashicorp/terraform/issues
   - Terraform Registry: https://registry.terraform.io/

4. **EKS Troubleshooting**:
   - AWS EKS Best Practices: https://aws.github.io/aws-eks-best-practices/

## ğŸ“š Additional Documentation

- [Secret Management](./SECRET_MANAGEMENT.md) - Comprehensive secret management strategy
- [Backup Strategy](./BACKUP_STRATEGY.md) - Backup and disaster recovery procedures
- [Runbook](./RUNBOOK.md) - Operational procedures and on-call guide

## ğŸ“– Architecture Decisions

### Why AWS over Other Cloud Providers?

1. **Mature EKS Service**: Most mature managed Kubernetes offering
2. **Graviton Processors**: ARM-based instances for cost savings
3. **Comprehensive Services**: Best-in-class managed services (RDS, S3, etc.)
4. **Global Presence**: Multiple regions including ap-south-1 (Mumbai)
5. **Enterprise Support**: 24/7 support and comprehensive documentation

### Why EKS over Self-Managed Kubernetes?

1. **Managed Control Plane**: No master node management
2. **Automatic Updates**: Seamless Kubernetes version upgrades
3. **AWS Integration**: Native integration with AWS services
4. **High Availability**: Multi-AZ control plane by default
5. **Cost Effective**: Control plane cost split across all workloads

### Why PostgreSQL over Other Databases?

1. **ACID Compliance**: Strong consistency guarantees
2. **Feature Rich**: JSON support, full-text search, extensions
3. **Open Source**: No licensing costs
4. **Community**: Large community and ecosystem
5. **Performance**: Excellent performance for OLTP workloads

### Why Grafana Stack (Loki + Mimir)?

1. **Unified Platform**: Single pane of glass for logs and metrics
2. **Cost Effective**: S3 backend much cheaper than alternatives
3. **Cloud Native**: Designed for Kubernetes
4. **Label-Based**: Efficient querying and filtering
5. **Open Source**: No vendor lock-in

### Why Terraform over Other IaC Tools?

1. **Provider Ecosystem**: Largest provider ecosystem
2. **State Management**: Robust state handling
3. **Module Reusability**: Easy to create and share modules
4. **Community**: Huge community and resources
5. **Multi-Cloud**: Works across all cloud providers

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/my-feature`
3. Make your changes
4. Test thoroughly
5. Commit: `git commit -am 'Add my feature'`
6. Push: `git push origin feature/my-feature`
7. Create a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see LICENSE file for details.

## ğŸ‘¥ Maintainers

- DevOps Team - devops@octabyte.com
- Security Team - security@octabyte.com

## ğŸ”– Version History

- **v1.0.0** (2024-01-15): Initial production release
- **v1.1.0** (2024-02-01): Added monitoring stack
- **v1.2.0** (2024-03-01): ARM-based instances for cost optimization

---

**Last Updated**: October 30, 2025
**Terraform Version**: >= 1.0
**EKS Version**: 1.33